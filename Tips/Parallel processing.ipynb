{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWuJ-Shsc4Eo",
        "colab_type": "text",
        "badge": true,
        "repo_name": "VinodKumar9576/TIPS",
        "branch": "master",
        "nb_path": "Tips/Parallel processing.ipynb",
        "comment": "This badge cell was added by colab-badge-action"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinodKumar9576/TIPS/blob/master/Tips/Parallel processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEngaLL8CXhW",
        "colab_type": "text"
      },
      "source": [
        "From somewhere in reddit\n",
        "> The VM used for Colaboratory appears to have 13GB RAM and 2 vCPU when checking using psutil (so a n1-highmem-2 instance)\n",
        "\n",
        "- 2-core Xeon 2.2GHz\n",
        "- 13GB RAM\n",
        "- 33GB HDD\n",
        "\n",
        "maximum lifetime of a VM is 12 hours. Idle VMs time out after 90m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMosqhC6Jv1K",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Define your function to apply on dataset\n",
        "2.   Define your parallel function\n",
        "3. Appy your function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAtn7smOGb4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1585732779572,
          "user_tz": -330,
          "elapsed": 1552,
          "user": {
            "displayName": "ml learning",
            "photoUrl": "",
            "userId": "11852164450280049527"
          }
        },
        "outputId": "81e4b8e7-7073-4ade-8837-7f48f7422e5d"
      },
      "source": [
        "from psutil import *\n",
        "\n",
        "cpu_count()\n",
        "# we have 2 cpu count so we can work parallely on 2 cores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4DYqbIVDKrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def txt2vec (df):\n",
        "  for qstn1 in tqdm(list(df),position=0):\n",
        "    doc1 = nlp(qstn1) #word2vec conversion using\n",
        "    mean_vec1 = np.zeros([len(doc1),300])\n",
        "    #making array of zeros of size of question\n",
        "    for word1 in doc1:\n",
        "      vec1 = word1.vector #getting the vector of each word in question\n",
        "      #vec1 = vec1.reshape(len(vec1),1)\n",
        "      try:\n",
        "        idf = tfidf_vect[str(word1)]\n",
        "      except:\n",
        "        idf = 0\n",
        "    \n",
        "      mean_vec1 +=vec1*idf\n",
        "    mean_vec1 = mean_vec1.mean(axis=0)\n",
        "    vecs1.append(mean_vec1)\n",
        "  \n",
        "  return vecs1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7xhE29wKnwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Parallelizing function\n",
        "\n",
        "def parallelization(df, func, n_cores=4):\n",
        "  df_split = np.array_split(df, n_cores)\n",
        "  pool = Pool(n_cores)\n",
        "  #applying func to each part of df\n",
        "  df = pd.concat(pool.map(func, df_split))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR2wPmEaPo0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex = parallelization(df,txt2vec)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}