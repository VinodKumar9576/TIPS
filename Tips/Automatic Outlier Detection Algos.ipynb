{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Automatic Outlier Detection Algos.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN0nVXP7FSr1VLJ4poNPPws"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5JnzlYNnjaRg","colab_type":"text"},"source":["{{ badge }}"]},{"cell_type":"markdown","metadata":{"id":"y_wPKWzTNFeu","colab_type":"text"},"source":["Ref: \n","https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/"]},{"cell_type":"markdown","metadata":{"id":"QMHnxhdHfMpX","colab_type":"text"},"source":["**<h1> Automatic Outlier Detection Algorithms**"]},{"cell_type":"markdown","metadata":{"id":"I1U0ktGxfW6k","colab_type":"text"},"source":["* The presence of outliers in a **classification or regression dataset** can result in **poort fit** and **lower** predictive modelling performance.\n","* Identifying and **removing outliers** is challenging with simple statistical methods given the large number of input variables in a dataset.\n","* In this notebook you'll find how to use automatic outlier detection and removal to improve machine learning predictive modelling performance.\n","* **Automatic outlier detection models** provide an alternative to satistical techniques with a large number of input variables with complex and unknown inter-relationships.\n","* We'll see how to correctly apply this technique and removal to the training dataset only to avoid data leakage.\n","*  How to evaluate and compare predictive modelling pipelines with outliers removed from the training dataset."]},{"cell_type":"markdown","metadata":{"id":"iJlslnczjfZN","colab_type":"text"},"source":["**<h2>Overview**\n","\n","1. Outlier Detection and Removal\n","2. Dataset and Performance Baseline\n","  1. House Price Regression Dataset\n","  2. Baseline Model Performance\n","3. Automatic Outlier Detection\n","  1. Isolation Forest\n","  2. Minimum Covariance Determinant\n","  3. Local Outlier Factor\n","  4. One-Class SVM"]},{"cell_type":"markdown","metadata":{"id":"EmiGs95uj7we","colab_type":"text"},"source":["**<h2>Outlier Detection and Removal**"]},{"cell_type":"markdown","metadata":{"id":"qOEEygjfmsBr","colab_type":"text"},"source":["* Most common type of outlier is the observation that are far from the rest of the observations or the center of mass of observations.\n","* This is easy to understand when we have one or two variables and we can visualize the data as histogram or scatter plot. But in case of many input features, task becomes more challenging.\n","* Simple statistical methods for identifying outliers use **standard deviations** or the **interquartile range**.\n","* Outliers can skew statistical measures and data distributions, providing a misleading representation of the underlying data and relationships.\n","* Removing outliers from training data before modeling can result in better fit of the data and in turn more skillful predictions.\n","* Outlier detection algorithms that we are going to discuss approach the definition of an outlier in slighter different ways."]},{"cell_type":"markdown","metadata":{"id":"uMj7AC60okGz","colab_type":"text"},"source":["**<h2>Dataset and Performance Baseline**"]},{"cell_type":"markdown","metadata":{"id":"fpYXUhzTo3VL","colab_type":"text"},"source":["**<h3> House Price Regression Dataset**"]},{"cell_type":"markdown","metadata":{"id":"X1ZXomDYpJmv","colab_type":"text"},"source":["* We'll use **Boston house price regression** dataset for the rest of the notebook."]},{"cell_type":"code","metadata":{"id":"iyvmG-f2edgh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595768375176,"user_tz":-330,"elapsed":3874,"user":{"displayName":"ml learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi54U31iecMtBQhQx-mLOAQPIRpb8LNaNz7bB7A=s64","userId":"11852164450280049527"}},"outputId":"e2ecf415-37bd-40ca-a859-a9b9ba42755b"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n","df = pd.read_csv(url, header = None)\n","\n","data = df.values\n","\n","#splitting the dataset into input and output variables\n","X, y = data[:, :-1], data[:, -1]\n","\n","print(X.shape, y.shape)\n","\n","#Splitting the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1)\n","\n","#summarize the shape of the train and test sets\n","print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["(506, 13) (506,)\n","(339, 13) (167, 13) (339,) (167,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jYlPSYqyteda","colab_type":"text"},"source":["**<h3>Baseline Model Performance**"]},{"cell_type":"markdown","metadata":{"id":"xQioEab6tl-w","colab_type":"text"},"source":["* We are dealing with **regression** dataset meaning we will be predicting a numeric value.\n","* We'll fit a linear regression algorithm and evaluate predictions using the **Mean absolute Error (MAE)**."]},{"cell_type":"code","metadata":{"id":"Xl9am7XRtbgF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595768375183,"user_tz":-330,"elapsed":3861,"user":{"displayName":"ml learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi54U31iecMtBQhQx-mLOAQPIRpb8LNaNz7bB7A=s64","userId":"11852164450280049527"}},"outputId":"e813da47-4aa8-4132-fe0a-952be8d0891e"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error\n","\n","#fitting the model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","#evaluating the model\n","y_pred = model.predict(X_test)\n","\n","#evaluating the predictions\n","mae = mean_absolute_error(y_test, y_pred)\n","print('MAE : %.3f'% mae)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["MAE : 3.417\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iDQm-uZwu0nG","colab_type":"text"},"source":["**<h2>Automatic Outlier Detection**"]},{"cell_type":"markdown","metadata":{"id":"rf2wUepCu44Z","colab_type":"text"},"source":["* In scikit-learn, we can find many built-in automatic methods for identifying outliers in data.\n","* In each method, fit model will predict which examples in the training dataset are outliers and which are not.\n","* The outliers will then be removed from the training dataset, then the model will be fit on the remaining examples and evaluated on the entire test dataset.\n","* We do not fit the outlier detection method on the entire training dataset as this would lead to **data leakage**."]},{"cell_type":"markdown","metadata":{"id":"dIA505wUwS0U","colab_type":"text"},"source":["**<h3> Isolation Forest**"]},{"cell_type":"markdown","metadata":{"id":"xL78g1r-wWT7","colab_type":"text"},"source":["* It is a tree-based anamoly detection algorithm.\n","* It isolates anamolies that are **few in number** and **different in the feature space**.\n","* **IsolationForest class** in scikit-learn library is used.\n","* **contamination** is the most important hyperparameter in the model, used to help estimate the number of outliers in the dataset.\n","* This value ranges between **0.0 - 0.5** and by default set to **0.1**."]},{"cell_type":"code","metadata":{"id":"EVpeBEcxuuEP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595768375185,"user_tz":-330,"elapsed":3849,"user":{"displayName":"ml learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi54U31iecMtBQhQx-mLOAQPIRpb8LNaNz7bB7A=s64","userId":"11852164450280049527"}},"outputId":"cac3a9c0-00be-47ff-ea28-74f21c4f0621"},"source":["from sklearn.ensemble import IsolationForest\n","\n","#identifying outliers in the training dataset\n","iso = IsolationForest(contamination = 0.1)\n","y_pred = iso.fit_predict(X_train)\n","\n","#select all rows that are not outliers\n","mask = y_pred != -1\n","\n","X_train_isf, y_train_isf = X_train[mask, :], y_train[mask]\n","\n","#shape of updated training dataset\n","print(X_train_isf.shape, y_train_isf.shape)\n","\n","#fit the model\n","model = LinearRegression()\n","model.fit(X_train_isf, y_train_isf)\n","\n","y_pred = model.predict(X_test)\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","print('MAE : %.3f' % mae)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(305, 13) (305,)\n","MAE : 3.224\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sUbC_veOypP7","colab_type":"text"},"source":["* In this case, we can see that the model identified and removed 34 outliers and achieved a MAE of about **3.188** which is less than previous value **3.417**.\n","* Results may differ given the stochastic nature of the learning algorithm, the evaluation procedure, and/or differences in precision across systems. Try running example a few times."]},{"cell_type":"markdown","metadata":{"id":"hsQsw0Coy76s","colab_type":"text"},"source":["**<h3>Minimum Covariance Determinant**"]},{"cell_type":"markdown","metadata":{"id":"H9-N75T2zbWf","colab_type":"text"},"source":["* If the input features have a **Gaussian distribution**, then simple statistical methods can be used to detect outliers.\n","* For example, if the dataset has two input variables and both are Gaussian, then the feature space forms a multi-dimensional Gaussian.\n","* The knowledge of such distribution can be used to identify values far from the distribution.\n","* MCD approach can be generalized by defining a **hypersphere (ellipsoid)** that covers the normal data, and data that falls outside this shape is considered an **outlier**. An efficient implementation of this technique for multivariate data is known as the **Minimum Covariance Determinant or MCD**.\n","* Scikit-learn library provides access to this method via the **EllipticEnvelope** class.\n","* In this method also there's a hyper parameter **contamination** that defines the expected ratio of outliers to be observed in practice.\n","* We'll set it to a value of **0.01**, found with little trial and error."]},{"cell_type":"code","metadata":{"id":"Mvi1hO9NySt-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595768375188,"user_tz":-330,"elapsed":3836,"user":{"displayName":"ml learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi54U31iecMtBQhQx-mLOAQPIRpb8LNaNz7bB7A=s64","userId":"11852164450280049527"}},"outputId":"af4c9ff1-3310-490b-b6aa-aee0e7830b9c"},"source":["from sklearn.covariance import EllipticEnvelope\n","\n","#identifying outliers in the training dataset\n","ee = EllipticEnvelope(contamination = 0.01)\n","y_pred = ee.fit_predict(X_train)\n","\n","#select all rows that are not outliers\n","mask = y_pred!=-1\n","X_train_mcd, y_train_mcd = X_train[mask, :], y_train[mask]\n","\n","#shape of updated trainin dataset\n","print(X_train_mcd.shape, y_train_mcd.shape)\n","\n","#fit the model\n","model = LinearRegression()\n","model.fit(X_train_mcd, y_train_mcd)\n","\n","#evaluate the model\n","y_pred = model.predict(X_test)\n","\n","#evaluate predictions\n","mae = mean_absolute_error(y_test, y_pred)\n","print('MAE : %.3f'% mae)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(335, 13) (335,)\n","MAE : 3.388\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jLgFxzYn2_rQ","colab_type":"text"},"source":["* In this case, we can see that the model identified and removed 4 outliers and achieved a MAE of about **3.388** which is less than previous value **3.417**.\n","* Results may differ given the stochastic nature of the learning algorithm, the evaluation procedure, and/or differences in precision across systems. Try running example a few times."]},{"cell_type":"markdown","metadata":{"id":"c5I0lPMOPMxn","colab_type":"text"},"source":["**<h3>Local Outlier Factor**"]},{"cell_type":"markdown","metadata":{"id":"rN8CC6bPPSTo","colab_type":"text"},"source":["* A simple approach to identify outliers is to locate points that are far from the other examples in the feature space.\n","* This method works well for feature spaces with low dimensionality, but becomes less reliable as the number of features is increased -- referred to as the **curse of dimensionality**.\n","* The local outlier factor(LOF) is a technique that harnesses the idea of **nearest neighbors** for outlier detection.\n","* Each example is assigned a scoring of **how isolated** or **how likely is to be outlier** based on the size of its local neighborhood.\n","* Those examples with the **largest score** are more likely to be outliers.\n","* Scikit-learn library provides an implementation of this approach in the **LocalOutlierFactor** class.\n","* Hyper-parameter **contamination** is the expected percentage of outliers in the dataset. Default value 0.1"]},{"cell_type":"code","metadata":{"id":"ZciQ0qx_15L_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595768375190,"user_tz":-330,"elapsed":3821,"user":{"displayName":"ml learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi54U31iecMtBQhQx-mLOAQPIRpb8LNaNz7bB7A=s64","userId":"11852164450280049527"}},"outputId":"4ef594b9-a436-484c-ea7a-71c0589b62a6"},"source":["from sklearn.neighbors import LocalOutlierFactor\n","\n","\n","#identify outliers in the training dataset\n","lof = LocalOutlierFactor()\n","y_pred = lof.fit_predict(X_train)\n","\n","#select all rows that are not outliers\n","mask = y_pred != -1\n","X_train_lof, y_train_lof = X_train[mask, :], y_train[mask]\n","\n","print(X_train_lof.shape, y_train_lof.shape)\n","\n","#fit the model\n","model = LinearRegression()\n","model.fit(X_train_lof, y_train_lof)\n","\n","#evaluate model\n","y_pred = model.predict(X_test)\n","\n","#evaluate predictions\n","mae = mean_absolute_error(y_test, y_pred)\n","print('MAE : %.3f' % mae)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(305, 13) (305,)\n","MAE : 3.356\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z9mCsBiIUcfT","colab_type":"text"},"source":["**<h3>One-Class SVM**"]},{"cell_type":"markdown","metadata":{"id":"ht6xKkDXUgbS","colab_type":"text"},"source":["* The SVM algorithm developed initially for binary classification and can be used for **one-class** classification.\n","* When modelling one class, the algorithm captures the **density of the majority class** and classifies points on the extremes of the density function as outliers.\n","* Although SVM is a classification algorithm and **One-class SVM** is also a classification algorithm, it can be used to identify outliers in input data for both **regression** and **classification** datasets.\n","* **OneClassSVM** is the class provided by scikit-learn for implementation of one-class SVM.\n","* Hyper parameter is 'nu' that specifies the approximate ratio of outliers in the dataset, which defaults to 0.1.\n","* In this case we keep it to 0.01, found with little trial and error."]},{"cell_type":"code","metadata":{"id":"WdFrfY-sT9vy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595768792927,"user_tz":-330,"elapsed":1671,"user":{"displayName":"ml learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi54U31iecMtBQhQx-mLOAQPIRpb8LNaNz7bB7A=s64","userId":"11852164450280049527"}},"outputId":"a25ff1e7-d7a1-4748-f5cd-56c512670134"},"source":["from sklearn.svm import OneClassSVM\n","\n","#identify outliers in the training dataset.\n","ee = OneClassSVM(nu = 0.01)\n","y_pred = ee.fit_predict(X_train)\n","\n","mask = y_pred != -1\n","\n","X_train_ocsvm, y_train_ocsvm = X_train[mask, :], y_train[mask]\n","\n","print(X_train_ocsvm.shape, y_train_ocsvm.shape)\n","\n","#fit the model\n","model = LinearRegression()\n","model.fit(X_train_ocsvm, y_train_ocsvm)\n","\n","#evaluate the model\n","y_pred = model.predict(X_test)\n","\n","#evaluate predictions\n","mae = mean_absolute_error(y_test, y_pred)\n","print('MAE : %.3f' %mae)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(336, 13) (336,)\n","MAE : 3.431\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oka61TkTO45x","colab_type":"text"},"source":["* In this case, we can see that only three outliers were identified and removed and the model achieved a MAE of about 3.431, which is not better than the baseline model that achieved 3.417. Perhaps better performance can be achieved with more tuning."]},{"cell_type":"code","metadata":{"id":"UeKR8Hg0O2Aj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}