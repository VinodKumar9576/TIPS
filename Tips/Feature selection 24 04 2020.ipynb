{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature selection 24/04/2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZfXI/kupk9nHmMNTSFkfd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvQy_SFycKu5",
        "colab_type": "text",
        "badge": true,
        "repo_name": "VinodKumar9576/TIPS",
        "branch": "master",
        "nb_path": "Tips/Feature selection 24 04 2020.ipynb",
        "comment": "This badge cell was added by colab-badge-action"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinodKumar9576/TIPS/blob/master/Tips/Feature selection 24 04 2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z4zkri-90Bt",
        "colab_type": "text"
      },
      "source": [
        "#Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRRSpSIv-B5Z",
        "colab_type": "text"
      },
      "source": [
        "* Feature selection also called variable selection or attribute selection.\n",
        "* Automatic selection of feat. that are most relevant to the model you are working on.\n",
        "* Selectinng a subset of relevant features for use in model construction.\n",
        "* Different from dim. reduction, as dim reduction method do so by **creating new combination of feat.** whereas feat. selection **include and exclude feat.** present in data without changing them but both seek to reduce the no. of feat.\n",
        "* Dim red : PCA, SVD and Sammon's mapping\n",
        "* Feat selection mostly **acts as a filter**, muting out features that aren't useful in addition to your existing feats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4tXetUg_uMv",
        "colab_type": "text"
      },
      "source": [
        "**Problems Feat. Selection solves**\n",
        "\n",
        "* Helps in creating accurate predictive model by choosing features that gives you better/good accuracy whilst requiring less data.\n",
        "* Identifies and removes unneeeded,irrelevant,redundant feat that don't contribute to the acc. or may infact decrease it.\n",
        "* Objective is three fold: \n",
        "  * Improving the prediction perfomance of model\n",
        "  * Providing faster and more cost-effective models\n",
        "  * Providing better understanging of underlying process that generated the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-xZ1ZLvCKrQ",
        "colab_type": "text"
      },
      "source": [
        "**Feature Selection ALgorithms**\n",
        "1. Filter Methods:\n",
        "  * These methods apply stastical measure to assign a score to each feat.\n",
        "  * Feat ranked by score to be either removed or kept from dataset.\n",
        "  * Often univariate and consider feat independently or with regard to dependant variable.\n",
        "  * Eg: Chi squared test, information gain, correlation coefficient scores.\n",
        "\n",
        "2. Wrapper Methods:\n",
        "  * Consider selection of a set of feat. as a search problem.\n",
        "  * Different combinations are prepared, eavaluated and compared to other combinations.\n",
        "  * A predictive model is used to evaluate a combination of feat. and assign a score based on model accuracy.\n",
        "  * Search process : methodical - best-first search, stochastic - random hill climbing, heuristic - forward/backward passes to add or remove feats.\n",
        "  * Eg: Recursive feature elimination algorithm , forward feature selection.\n",
        "\n",
        "3. Embedded Methods\n",
        "  * These methods learn which feats best contribute to model accuracy while the model is being created.\n",
        "  * Most common methods are regularization methods.\n",
        "  * Regularization methods also called as penalization methods that introduce additional constraints into the model that bias the model toward lowe complexity(fewer coefficients).\n",
        "  * Eg : LASSO, Elastic Net, Ridge Regression, Logistic Regression, Linear Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxP6o_2fGiSa",
        "colab_type": "text"
      },
      "source": [
        "* Don't do feature selection on entire training dataset, as CV also will be included in it. Which leads to biased model with good results when infact it is biased result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foVmwsZxG-0g",
        "colab_type": "text"
      },
      "source": [
        "#Feature selection for ML in python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2xnNU6UHXIZ",
        "colab_type": "text"
      },
      "source": [
        "**Three benefits of performing feature selection before modeling the data:**\n",
        "* `Reduces Overfitting : ` Less redundant data means less oppurtunity to make decisions based on noise.\n",
        "* `Improves Accuracy :` Less misleading data means modeling accuracy improves.\n",
        "* `Reduces Training Time :` Less data means that algorithms train faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuVLO-oMIqyI",
        "colab_type": "text"
      },
      "source": [
        "##Filter Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZPXexPkIxv4",
        "colab_type": "text"
      },
      "source": [
        "**Univariate Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tvbC5HnJAT2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1587748070184,
          "user_tz": -330,
          "elapsed": 9185,
          "user": {
            "displayName": "ml learning",
            "photoUrl": "",
            "userId": "11852164450280049527"
          }
        },
        "outputId": "ce4d8069-f740-4ad9-be5a-b9efe345c82e"
      },
      "source": [
        "#importing the dataset from kaggle\n",
        "\n",
        "import getpass as gt\n",
        "\n",
        "user = input('Provide Kaggle Username : ')\n",
        "api = gt.getpass('API key for '+ user +' :')\n",
        "print('')\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "kaggle_api_key = {'username':user, 'key':api}\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(kaggle_api_key, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d uciml/pima-indians-diabetes-database\n",
        "\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "with zipfile.ZipFile('/content/pima-indians-diabetes-database.zip', \"r\") as z:\n",
        "  dataframe = pd.read_csv(z.open('diabetes.csv'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory \u2018/root/.kaggle\u2019: File exists\n",
            "pima-indians-diabetes-database.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFAg-_I4I1FT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1587748537929,
          "user_tz": -330,
          "elapsed": 997,
          "user": {
            "displayName": "ml learning",
            "photoUrl": "",
            "userId": "11852164450280049527"
          }
        },
        "outputId": "847e9dbd-7d33-4e9b-a2a9-ce3eb9c3882f"
      },
      "source": [
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKBL-ajhKYrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1587749599661,
          "user_tz": -330,
          "elapsed": 1021,
          "user": {
            "displayName": "ml learning",
            "photoUrl": "",
            "userId": "11852164450280049527"
          }
        },
        "outputId": "e2dfa48f-928a-4226-c7b8-c028420d58b9"
      },
      "source": [
        "#feature selection with univariate statistical analysis\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "X = dataframe.iloc[:,0:8]\n",
        "Y = dataframe['Outcome']\n",
        "\n",
        "#feature extraction\n",
        "#k denotes number of features we want to keep\n",
        "test = SelectKBest(score_func = f_classif, k=4)\n",
        "fit = test.fit(X,Y)\n",
        "\n",
        "#summarize scores\n",
        "print('Scores of each feature from dataset')\n",
        "for i in range(len(X.columns)):\n",
        "  print(X.columns[i],' ', round(fit.scores_[i],3))\n",
        "  \n",
        "#retreiving k features that has highest scores\n",
        "features = fit.transform(X)\n",
        "\n",
        "#summarize selected features\n",
        "print('\\nFinal features')\n",
        "print(features[0:5,:])\n",
        "#it seems that feats.'Pregnancies,\tGlucose, BMI, Age' come out as result."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores of each feature from dataset\n",
            "Pregnancies   39.67\n",
            "Glucose   213.162\n",
            "BloodPressure   3.257\n",
            "SkinThickness   4.304\n",
            "Insulin   13.281\n",
            "BMI   71.772\n",
            "DiabetesPedigreeFunction   23.871\n",
            "Age   46.141\n",
            "\n",
            "Final features\n",
            "[[  6.  148.   33.6  50. ]\n",
            " [  1.   85.   26.6  31. ]\n",
            " [  8.  183.   23.3  32. ]\n",
            " [  1.   89.   28.1  21. ]\n",
            " [  0.  137.   43.1  33. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT89uA1wI1-K",
        "colab_type": "text"
      },
      "source": [
        "##Wrapper Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVElEkzlI33c",
        "colab_type": "text"
      },
      "source": [
        "**Forward feature selection or Recursive Feat Elimination**\n",
        "* This differs from **Permutation FI** in a way, that permutation FI just give you scores of all the features whereas Recursive Feat Elimination, eliminates the feat that doesn't contribute much to the model accuracy or in predicting the target attribute.\n",
        "* RFE works by recursively removing atttibutes and building a model on those feat. that remain.\n",
        "* Choice of algorithm doesn't matter as long as it is skillful and consistent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1V6IVo7I-LE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1587750095867,
          "user_tz": -330,
          "elapsed": 961,
          "user": {
            "displayName": "ml learning",
            "photoUrl": "",
            "userId": "11852164450280049527"
          }
        },
        "outputId": "a33c2e09-a5c2-4228-f227-b9213a9c6e99"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "X = dataframe.iloc[:,0:8]\n",
        "Y = dataframe['Outcome']\n",
        "\n",
        "#feature extracion\n",
        "model = LogisticRegression(max_iter=200)\n",
        "#selecting only top 3 features\n",
        "rfe = RFE(model, 3)\n",
        "fit = rfe.fit(X,Y)\n",
        "\n",
        "print(\"Num Features: %d\" % fit.n_features_)\n",
        "print(\"Selected Features: %s\" % fit.support_)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 6 5 1 1 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8RUOSi5P7xf",
        "colab_type": "text"
      },
      "source": [
        "* In `support_` array top selected features are marked as `TRUE`\n",
        "* In `ranking_` array top selected featues are marked as `1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjZm7TrQHCrY",
        "colab_type": "text"
      },
      "source": [
        "#How to choose a Feature selection method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp5MRkUoV1DP",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSABMEBoz0kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}