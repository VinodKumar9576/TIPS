{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Handling Missing data 09/05/2020.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPwxV3UkQ6dBGOM8YQt7ZOG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XUaga5hacQkN","colab_type":"text"},"source":["{{ badge }}"]},{"cell_type":"markdown","metadata":{"id":"-jWBIPO9EmrB","colab_type":"text"},"source":["You can refer the original post here:\n","\n","https://machinelearningmastery.com/handle-missing-data-python/"]},{"cell_type":"markdown","metadata":{"id":"EvRm683pDlsc","colab_type":"text"},"source":["**<h1>How to handle Missing Data with python**"]},{"cell_type":"markdown","metadata":{"id":"8pKev16kDuqu","colab_type":"text"},"source":["**<h3>Introduction**"]},{"cell_type":"markdown","metadata":{"id":"kgovA-zfDzy9","colab_type":"text"},"source":["* First of all why values will be missing in the dataset given?\n","* This can happen for various number of reasons such as observations not recorded at all may be due to cost issue for collecting, data got corrupted, data engineers didn't scrape the data well from sources.\n","* Handling missing values is such important as many ML algorithms do not tolerate data with missing values."]},{"cell_type":"markdown","metadata":{"id":"-qKbMKQyFGaN","colab_type":"text"},"source":["This post discusses below things:\n","  * Observing dataset\n","  * Mark missing values\n","  * Problems caused by missing values\n","  * Removing rows with missing values\n","  * Imputing missing values\n","  * Looking at algos that support missing values"]},{"cell_type":"markdown","metadata":{"id":"ybwfnD4BFmVp","colab_type":"text"},"source":["**<h3>1. Diabetes Dataset**"]},{"cell_type":"code","metadata":{"id":"7WVYY10rGZ9K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1589026705402,"user_tz":-330,"elapsed":17087,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"24197f98-43c2-40fa-f286-248730179089"},"source":["import getpass as gt\n","\n","user = input('Provide Kaggle Username : ')\n","api = gt.getpass('API key for '+ user +' :')\n","print('')\n","\n","\n","!pip install kaggle\n","!kaggle -v\n","\n","!mkdir ~/.kaggle\n","!touch ~/.kaggle/kaggle.json\n","\n","kaggle_api_key = {'username':user, 'key':api}\n","\n","import json\n","\n","with open('/root/.kaggle/kaggle.json','w') as file:\n","  json.dump(kaggle_api_key,file)\n","\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","!kaggle datasets download -d uciml/pima-indians-diabetes-database\n","\n","\n","import pandas as pd\n","from zipfile import ZipFile\n","\n","diabetes_zip = ZipFile('/content/pima-indians-diabetes-database.zip')\n","train_df = pd.read_csv(diabetes_zip.open('diabetes.csv'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 5, in <module>\n","    from kaggle.cli import main\n","  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n","    api.authenticate()\n","  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n","    self.config_file, self.config_dir))\n","IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n","mkdir: cannot create directory ‘/root/.kaggle’: File exists\n","Downloading pima-indians-diabetes-database.zip to /content\n","  0% 0.00/8.91k [00:00<?, ?B/s]\n","100% 8.91k/8.91k [00:00<00:00, 17.7MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qq_xiRnNHeH8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1589015495726,"user_tz":-330,"elapsed":1284,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"5b15c3dd-9b12-45ad-b1d9-6ce3f18c60eb"},"source":["train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n","0            6      148             72  ...                     0.627   50        1\n","1            1       85             66  ...                     0.351   31        0\n","2            8      183             64  ...                     0.672   32        1\n","3            1       89             66  ...                     0.167   21        0\n","4            0      137             40  ...                     2.288   33        1\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"oeUE5eruI2Sf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589006006815,"user_tz":-330,"elapsed":1359,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"ade702f7-d9a2-40c1-b049-c26f4f1ce373"},"source":["train_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(768, 9)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"L17siqTlJzth","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1589006265714,"user_tz":-330,"elapsed":1315,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"44945720-bd21-4f0a-8d79-114c8b7f60c7"},"source":["train_df['Outcome'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    500\n","1    268\n","Name: Outcome, dtype: int64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"hMq2IAYfI1YI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1589006587167,"user_tz":-330,"elapsed":1172,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"4cfb1bb6-a873-41b1-9b77-83833e28153a"},"source":["maj_cls = train_df['Outcome'][train_df['Outcome']==0].count()\n","min_cls = train_df['Outcome'][train_df['Outcome']==1].count()\n","tot_pts = train_df.shape[0]\n","\n","print('Size of dataset ' , tot_pts)\n","print('Majority class pionts ', maj_cls)\n","print('Minority class pionts ', min_cls)\n","print('Distribution of majority class (0) over data %.1f%%'%(maj_cls/tot_pts*100))\n","print('Distribution of minority class (1) over data %.1f%%'%(min_cls/tot_pts*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of dataset  768\n","Majority class pionts  500\n","Minority class pionts  268\n","Distribution of majority class (0) over data 65.1%\n","Distribution of minority class (1) over data 34.9%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8ixFYcqSIf5i","colab_type":"text"},"source":["* This is a binary classification dataset and distribution of data for each class is not balanced.\n","* Size of data is 768, 8 features and 1 output variable.\n","* If we can see the head data has value for some columns as '0' which indicates these are missing values."]},{"cell_type":"markdown","metadata":{"id":"j2reKmXlLlvB","colab_type":"text"},"source":["**<h3>2. Mark missing values**"]},{"cell_type":"markdown","metadata":{"id":"-mA7-CE5spP6","colab_type":"text"},"source":["* As the data size increases, no. of missing values also increases.\n","* We can use plots and summary statistics to identify missing or corrupt data."]},{"cell_type":"code","metadata":{"id":"mhQXOwklLkaH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1589015512974,"user_tz":-330,"elapsed":1714,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"674ab2a6-096a-4a10-a858-04788924fc9b"},"source":["train_df.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>3.845052</td>\n","      <td>120.894531</td>\n","      <td>69.105469</td>\n","      <td>20.536458</td>\n","      <td>79.799479</td>\n","      <td>31.992578</td>\n","      <td>0.471876</td>\n","      <td>33.240885</td>\n","      <td>0.348958</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>3.369578</td>\n","      <td>31.972618</td>\n","      <td>19.355807</td>\n","      <td>15.952218</td>\n","      <td>115.244002</td>\n","      <td>7.884160</td>\n","      <td>0.331329</td>\n","      <td>11.760232</td>\n","      <td>0.476951</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.078000</td>\n","      <td>21.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>99.000000</td>\n","      <td>62.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>27.300000</td>\n","      <td>0.243750</td>\n","      <td>24.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3.000000</td>\n","      <td>117.000000</td>\n","      <td>72.000000</td>\n","      <td>23.000000</td>\n","      <td>30.500000</td>\n","      <td>32.000000</td>\n","      <td>0.372500</td>\n","      <td>29.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>6.000000</td>\n","      <td>140.250000</td>\n","      <td>80.000000</td>\n","      <td>32.000000</td>\n","      <td>127.250000</td>\n","      <td>36.600000</td>\n","      <td>0.626250</td>\n","      <td>41.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>17.000000</td>\n","      <td>199.000000</td>\n","      <td>122.000000</td>\n","      <td>99.000000</td>\n","      <td>846.000000</td>\n","      <td>67.100000</td>\n","      <td>2.420000</td>\n","      <td>81.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Pregnancies     Glucose  ...         Age     Outcome\n","count   768.000000  768.000000  ...  768.000000  768.000000\n","mean      3.845052  120.894531  ...   33.240885    0.348958\n","std       3.369578   31.972618  ...   11.760232    0.476951\n","min       0.000000    0.000000  ...   21.000000    0.000000\n","25%       1.000000   99.000000  ...   24.000000    0.000000\n","50%       3.000000  117.000000  ...   29.000000    0.000000\n","75%       6.000000  140.250000  ...   41.000000    1.000000\n","max      17.000000  199.000000  ...   81.000000    1.000000\n","\n","[8 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"p6YIeCpYtQYK","colab_type":"text"},"source":["* From above description we can see that minimum of some columns is '0' which is missing or corrupted data.\n","* We are going to confirm this by looking at raw data."]},{"cell_type":"code","metadata":{"id":"aMs_q9zmtxjh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1589015783190,"user_tz":-330,"elapsed":1016,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"9d976c54-5b29-4af1-e686-537a0e6ed7d6"},"source":["num_missing = (train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']]==0).sum()\n","\n","print(num_missing)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pregnancies      111\n","Glucose            5\n","BloodPressure     35\n","SkinThickness    227\n","Insulin          374\n","BMI               11\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VsnpoTxZuRgh","colab_type":"text"},"source":["* We can see that columns 'Glucose','BloodPressure','BMI' have few points as zero whereas columns 'Insulin' has half of the rows as zeros and other columns as well.\n","* We can't just simply delete those rows as we'll loose most of the data, so we need some strategies to fill those columns with reasonable values.\n","* As we are seeing missing values as '0', we mark them with Nan(null) to identify easily.\n","* This marking helps us to replace them easily with reasonable values using some python functions."]},{"cell_type":"code","metadata":{"id":"iJEo-b1buHPR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1589016176726,"user_tz":-330,"elapsed":1313,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"11d47092-d27b-4db2-9d8a-5df562eb5020"},"source":["from numpy import nan\n","\n","train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,nan)\n","\n","#counting no. of nan values\n","print(train_df.isnull().sum())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pregnancies                 111\n","Glucose                       5\n","BloodPressure                35\n","SkinThickness               227\n","Insulin                     374\n","BMI                          11\n","DiabetesPedigreeFunction      0\n","Age                           0\n","Outcome                       0\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9PAGJPVVwJU7","colab_type":"text"},"source":["* Lets confirm this Nan values by looking at raw data"]},{"cell_type":"code","metadata":{"id":"2HgiJNaMvpP-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":669},"executionInfo":{"status":"ok","timestamp":1589016340743,"user_tz":-330,"elapsed":1117,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"8dd2c56a-fa4a-48ef-fcfe-3488f7189be0"},"source":["train_df.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.0</td>\n","      <td>148.0</td>\n","      <td>72.0</td>\n","      <td>35.0</td>\n","      <td>NaN</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>85.0</td>\n","      <td>66.0</td>\n","      <td>29.0</td>\n","      <td>NaN</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8.0</td>\n","      <td>183.0</td>\n","      <td>64.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>89.0</td>\n","      <td>66.0</td>\n","      <td>23.0</td>\n","      <td>94.0</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>137.0</td>\n","      <td>40.0</td>\n","      <td>35.0</td>\n","      <td>168.0</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5.0</td>\n","      <td>116.0</td>\n","      <td>74.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>25.6</td>\n","      <td>0.201</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3.0</td>\n","      <td>78.0</td>\n","      <td>50.0</td>\n","      <td>32.0</td>\n","      <td>88.0</td>\n","      <td>31.0</td>\n","      <td>0.248</td>\n","      <td>26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>10.0</td>\n","      <td>115.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>35.3</td>\n","      <td>0.134</td>\n","      <td>29</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2.0</td>\n","      <td>197.0</td>\n","      <td>70.0</td>\n","      <td>45.0</td>\n","      <td>543.0</td>\n","      <td>30.5</td>\n","      <td>0.158</td>\n","      <td>53</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8.0</td>\n","      <td>125.0</td>\n","      <td>96.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.232</td>\n","      <td>54</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.0</td>\n","      <td>110.0</td>\n","      <td>92.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>37.6</td>\n","      <td>0.191</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>10.0</td>\n","      <td>168.0</td>\n","      <td>74.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>38.0</td>\n","      <td>0.537</td>\n","      <td>34</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>10.0</td>\n","      <td>139.0</td>\n","      <td>80.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>27.1</td>\n","      <td>1.441</td>\n","      <td>57</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1.0</td>\n","      <td>189.0</td>\n","      <td>60.0</td>\n","      <td>23.0</td>\n","      <td>846.0</td>\n","      <td>30.1</td>\n","      <td>0.398</td>\n","      <td>59</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>5.0</td>\n","      <td>166.0</td>\n","      <td>72.0</td>\n","      <td>19.0</td>\n","      <td>175.0</td>\n","      <td>25.8</td>\n","      <td>0.587</td>\n","      <td>51</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7.0</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>30.0</td>\n","      <td>0.484</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>NaN</td>\n","      <td>118.0</td>\n","      <td>84.0</td>\n","      <td>47.0</td>\n","      <td>230.0</td>\n","      <td>45.8</td>\n","      <td>0.551</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.0</td>\n","      <td>107.0</td>\n","      <td>74.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>29.6</td>\n","      <td>0.254</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1.0</td>\n","      <td>103.0</td>\n","      <td>30.0</td>\n","      <td>38.0</td>\n","      <td>83.0</td>\n","      <td>43.3</td>\n","      <td>0.183</td>\n","      <td>33</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1.0</td>\n","      <td>115.0</td>\n","      <td>70.0</td>\n","      <td>30.0</td>\n","      <td>96.0</td>\n","      <td>34.6</td>\n","      <td>0.529</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n","0           6.0    148.0           72.0  ...                     0.627   50        1\n","1           1.0     85.0           66.0  ...                     0.351   31        0\n","2           8.0    183.0           64.0  ...                     0.672   32        1\n","3           1.0     89.0           66.0  ...                     0.167   21        0\n","4           NaN    137.0           40.0  ...                     2.288   33        1\n","5           5.0    116.0           74.0  ...                     0.201   30        0\n","6           3.0     78.0           50.0  ...                     0.248   26        1\n","7          10.0    115.0            NaN  ...                     0.134   29        0\n","8           2.0    197.0           70.0  ...                     0.158   53        1\n","9           8.0    125.0           96.0  ...                     0.232   54        1\n","10          4.0    110.0           92.0  ...                     0.191   30        0\n","11         10.0    168.0           74.0  ...                     0.537   34        1\n","12         10.0    139.0           80.0  ...                     1.441   57        0\n","13          1.0    189.0           60.0  ...                     0.398   59        1\n","14          5.0    166.0           72.0  ...                     0.587   51        1\n","15          7.0    100.0            NaN  ...                     0.484   32        1\n","16          NaN    118.0           84.0  ...                     0.551   31        1\n","17          7.0    107.0           74.0  ...                     0.254   31        1\n","18          1.0    103.0           30.0  ...                     0.183   33        0\n","19          1.0    115.0           70.0  ...                     0.529   32        1\n","\n","[20 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"aOHbQrTWwWHP","colab_type":"text"},"source":["**<h3>3. Missing Values Causes Problems**"]},{"cell_type":"markdown","metadata":{"id":"Gy_Uk08ewnQf","colab_type":"text"},"source":["* Due to this existing missing values in dataset cause some Machine learning algorithms go insane.\n","* Most missing values are common in real world data and unfortunately many ml algorithms can't handle these. Therefore, this problem needs to be addressed prior to modelling.\n","* To check the errors, we will try Linear Discriminant Analysis (LDA) algorithm on the dataset with missing values.\n","* We tried this particular algorithm, because LDA doesn't work with data having missing values.\n","* We already marked the missing values before, we use the same dataset for analysis.\n","* We evaluate LDA using 3-fold cv and print mean ***accuracy***."]},{"cell_type":"code","metadata":{"id":"srYW1El-wSPu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1589017024022,"user_tz":-330,"elapsed":1264,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"c71e8a1c-b124-483f-d998-d602d2bf3d35"},"source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","\n","#splitting dataset into inputs and outputs\n","values = train_df.values\n","X = values[:,0:8]\n","y = values[:,8]\n","\n","#defining model\n","model_LDA = LinearDiscriminantAnalysis()\n","\n","cv = KFold(n_splits=3, shuffle=True, random_state=1)\n","\n","#evaluating the model\n","result = cross_val_score(model_LDA, X,y, cv=cv, scoring = 'accuracy')\n","\n","#report the mean performance\n","print('Accuracy: %.3f' %result.mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: nan\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n","\n","  FitFailedWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MgwA7VsLzCpG","colab_type":"text"},"source":["* It is clear from above error that missing values creates errors in some models."]},{"cell_type":"markdown","metadata":{"id":"h08xJbelzJiI","colab_type":"text"},"source":["**<h3>4. Remove rows with Missing Values**"]},{"cell_type":"markdown","metadata":{"id":"Fu3RsEoKXP_z","colab_type":"text"},"source":["* Most simplest way for handling missing data.\n","* We remove entire row that if a column of it contains missing value using ***dropna()***."]},{"cell_type":"code","metadata":{"id":"mwODZKU8y3oR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1589027121780,"user_tz":-330,"elapsed":1254,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"dbf8ae4a-dba9-4a43-c723-7ecff69c3e5c"},"source":["from numpy import nan\n","\n","train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,nan)\n","\n","#dropping rows with missing values\n","train_df_rm = train_df.dropna()\n","print('Shape before dropping : ', train_df.shape)\n","print('Shape after dropping : ',train_df_rm.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape before dropping :  (768, 9)\n","Shape after dropping :  (336, 9)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AuPUgAvyZdcp","colab_type":"text"},"source":["* Now we use the modified dataset to test LDA algorithm."]},{"cell_type":"code","metadata":{"id":"MLzveOP0ZLIQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589027167296,"user_tz":-330,"elapsed":1351,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"788ce9c3-4751-4905-db6f-6ac61846773d"},"source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","\n","#splitting dataset into inputs and outputs\n","values = train_df_rm.values\n","X = values[:,0:8]\n","y = values[:,8]\n","\n","#defining model\n","model_LDA = LinearDiscriminantAnalysis()\n","\n","cv = KFold(n_splits=3, shuffle=True, random_state=1)\n","\n","#evaluating the model\n","result = cross_val_score(model_LDA, X,y, cv=cv, scoring = 'accuracy')\n","\n","#report the mean performance\n","print('Accuracy: %.3f' %result.mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.777\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pliFPU-cZnYo","colab_type":"text"},"source":["* The example run successfully and accuracy got returned.\n","* But removing the rows is not a good idea all the time as in the current case we lost 50% of data.\n","* So, following we are going to look at some imputation techniques."]},{"cell_type":"markdown","metadata":{"id":"yObB0MfQaQ1Y","colab_type":"text"},"source":["**<h3>5. Impute Missing Values**"]},{"cell_type":"markdown","metadata":{"id":"18GWxITTajhH","colab_type":"text"},"source":["* Imputing referes to using a model to replace the missing values.\n","* We can use training set predictors to estimate the values of other predictors.\n","* Options we have for imputation:\n","  * A constant value that has meaning within the domain, eg: 0 (distinct from all other values)\n","  * A value from another randomly selected record.\n","  * A mean, median or mode value for the column.\n","  * A value estimated by another predictive model.\n","* Any method we choose to perfom on training model will have to be performed on unseen test data as well if test data has also missing values.\n","* Eg: If we choose to impute mean column values, these column values will need to be stored to file for later use on new data that has missin values.\n","* We use ***fillna()*** method to impute missin values."]},{"cell_type":"code","metadata":{"id":"EyUW691faQWd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1589028989578,"user_tz":-330,"elapsed":2110,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"f03e627d-b350-4128-994d-fdc8eb14d15e"},"source":["from numpy import nan\n","\n","#load dataset\n","train_df = pd.read_csv(diabetes_zip.open('diabetes.csv'))\n","\n","#marking missing values\n","train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,nan)\n","\n","#imputing missing values with mean column values\n","train_df.fillna(train_df.mean(), inplace=True)\n","\n","#count number of missing values in each column\n","print(train_df.isnull().sum())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pregnancies                 0\n","Glucose                     0\n","BloodPressure               0\n","SkinThickness               0\n","Insulin                     0\n","BMI                         0\n","DiabetesPedigreeFunction    0\n","Age                         0\n","Outcome                     0\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kciQU639gxrd","colab_type":"text"},"source":["* The above procedure has a risk of data leakage where we did imputation before splitting the data for testing and cross validation.\n","* So we use ***Pipeline*** where data is first passed through the imputer transform such as ***SimpleImputer*** then provided to the model.\n","* This way we can be sure that imputer and model are both fit only on the training dataset and evaluated on test dataset within each cross-validation fold thus avoiding data leakage."]},{"cell_type":"code","metadata":{"id":"ClTICoolf04E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1589028820641,"user_tz":-330,"elapsed":2024,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"d55eb240-cabb-431b-ed0a-679433aa1288"},"source":["from numpy import nan\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","\n","#load dataset\n","train_df = pd.read_csv(diabetes_zip.open('diabetes.csv'))\n","\n","#marking missing values\n","train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = train_df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,nan)\n","\n","\n","#splitting dataset into inputs and outputs\n","values = train_df_rm.values\n","X = values[:,0:8]\n","y = values[:,8]\n","\n","#define the imputer\n","imputer = SimpleImputer(mising_values=nan, strategy='mean')\n","\n","#defining model\n","model_LDA = LinearDiscriminantAnalysis()\n","\n","#defining the model pipeline\n","pipeline = Pipeline(steps=[('imputer', imputer), ('model', model_LDA)])\n","\n","kfold = KFold(n_splits=3, shuffle=True, random_state=1)\n","\n","#evaluating the model\n","result = cross_val_score(pipeline, X,y, cv=kfold, scoring = 'accuracy')\n","\n","#report the mean performance\n","print('Accuracy: %.3f' %result.mean())\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pregnancies                   4.494673\n","Glucose                     121.686763\n","BloodPressure                72.405184\n","SkinThickness                29.153420\n","Insulin                     155.548223\n","BMI                          32.457464\n","DiabetesPedigreeFunction      0.471876\n","Age                          33.240885\n","Outcome                       0.348958\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"TeIaSWF8jMXh","colab_type":"text"},"source":["**<h3>6. Algorithms that Support Missing Values**"]},{"cell_type":"markdown","metadata":{"id":"CtKR8VtWjY1h","colab_type":"text"},"source":["* Not all algorithms fail when there's missing data.\n","* Eg: KNN ignore a column from a distance measure when a value is missing.<br>NB also support missing values when makind a prediction.\n","* There are also algorithms that use missinng values as a unique and different value when building the predictive model such as *classification* and *regression trees*.\n","* But scikit-learn implementation of naive bayes, DT and KNN are not robust to missing values."]},{"cell_type":"markdown","metadata":{"id":"9QaV73BVk9KE","colab_type":"text"},"source":["**<h3>7. Imputing missing values using model predictions**"]},{"cell_type":"markdown","metadata":{"id":"Wmuzw4dx7XQ8","colab_type":"text"},"source":["* This is one of the best and most efficient method for handling missing data.\n","* Depending on the class of data that is missing, we either use classification or regression model to predict missing data.\n","* We turn features with missing values as 'Output', making rest of the features (with all the columns data available) as predicting columns.\n","* Call the feature that has missing values as *y*.\n","* Split the data into sets with missing values and without missing values. Name the missing set 'X_test' and without missing 'X_train' and take *y* off the second set, naming it as y_train.\n","* After predicting missing values, add it to X_test as your y_test column, then combine sets together.\n","* The only drawback to this approach is that if there is no correlation between features with missing data and other features which we use for prediction, then the model will be bias for predicting missing values."]},{"cell_type":"code","metadata":{"id":"2in4pAVU9KAW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}