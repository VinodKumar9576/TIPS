{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spark parallelisation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1_i61OZpXIxbL5w9vNUPLZ1noF8aMYAk_","authorship_tag":"ABX9TyOWYJKqCYqbRAFVOyYkujPP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FQaXEt9UdCK9","colab_type":"text"},"source":["{{badge}}"]},{"cell_type":"markdown","metadata":{"id":"wi3fJDIPJvF6","colab_type":"text"},"source":["* In this we do parameter searching using parallelisation.\n","* i.e entire data set is run parallely among all the parameters and return the best."]},{"cell_type":"markdown","metadata":{"id":"VtjgpzvypKir","colab_type":"text"},"source":["* When working with large data consider below settings"]},{"cell_type":"markdown","metadata":{"id":"e-sBQnjGjqFE","colab_type":"text"},"source":["##Using spark parallelization"]},{"cell_type":"markdown","metadata":{"id":"EjnE07afj0DI","colab_type":"text"},"source":["* Replicating the data\n","* We need to work on the original sampled data."]},{"cell_type":"markdown","metadata":{"id":"D-8UIpgGkvRk","colab_type":"text"},"source":["* **Replicating the data twice lets say as we have 2 cpus.**"]},{"cell_type":"markdown","metadata":{"id":"HLJf20n6nmhe","colab_type":"text"},"source":["* When converting pandas dataframe to spark dataframe, its taking more time.\n","* To optimize it we use spark apache arrow.\n","* By default it's disabled, we enable it now.\n","* Not all Spark data types are supported and an error can be raised if a column has an unsupported type. If an error occurs during `createDataFrame()`, Spark falls back to create the DataFrame without Arrow."]},{"cell_type":"markdown","metadata":{"id":"ICpdJ-yjocIo","colab_type":"text"},"source":["\n","\n","```\n","# Enable Arrow-based columnar data transfers\n","spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n","\n","# Generate a pandas DataFrame\n","pdf = pd.DataFrame(np.random.rand(100, 3))\n","\n","# Create a Spark DataFrame from a pandas DataFrame using Arrow\n","df = spark.createDataFrame(pdf)\n","\n","# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n","result_pdf = df.select(\"*\").toPandas()\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dIm8C0ZR3Pst","colab_type":"text"},"source":["\n","\n","```\n","from pyspark import SparkConf, SparkContext\n","spark.stop()\n","conf = SparkConf().setAppName(\"App\")\n","conf = (conf.setMaster('local[*]')\n","        .set('spark.executor.memory', '4G')\n","        .set('spark.driver.memory', '45G')\n","        .set('spark.driver.maxResultSize', '10G'))\n","sc = SparkContext(conf=conf)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"fNeE6NUynmHa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"error","timestamp":1587190222007,"user_tz":-330,"elapsed":748010,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"9b3400c0-1b76-41ac-d3f4-63bc87700c54"},"source":["# Enable Arrow-based columnar data transfers\n","#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n","\n","#making sampled data as spark dataframe\n","fnl_ft_samp_sp = spark.createDataFrame(fnl_ft_samp)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-d9489e4f4372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfnl_ft_samp_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnl_ft_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_numpy_record_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonParallelizeServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize_to_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                     \u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;31m# we eagerily reads the file so we can delete right after.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mreader_func\u001b[0;34m(temp_filename)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadRDDFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n"]}]},{"cell_type":"code","metadata":{"id":"048TJtngjwMo","colab_type":"code","colab":{}},"source":["#Replicating the spark dataframe into multiple copies\n","replication_df = spark.createDataFrame(pd.DataFrame(list(range(0,2)), columns =['replication_id']))\n","\n","#cross joining with features dataframe\n","fnl_ft_rep_df = df.crossJoin(replication_df)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yyRzRSHquMe2","colab_type":"text"},"source":["* **Creating Pandas UDF to run the model.**"]},{"cell_type":"code","metadata":{"id":"pMpPzVBpmS6U","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvId67eWBFD7","colab_type":"code","colab":{}},"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#use latest spark version\n","!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n","!pip install -q findspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8TN9WO3BU0M","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pja8ltlpEQbj","colab_type":"code","colab":{}},"source":["import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8J-9VhDnBYLs","colab_type":"code","colab":{}},"source":["from sklearn import datasets\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import pyspark.sql.functions as F\n","import random\n","from pyspark.sql.types import *\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdd8H4KKB-k7","colab_type":"code","colab":{}},"source":["X,y = datasets.make_classification(n_samples=10000, n_features=4, n_informative=2, n_classes=2, random_state=1,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVJpY-WOCBkZ","colab_type":"code","colab":{}},"source":["train = pd.DataFrame(X)\n","train['target'] = y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JUsbG1RCCGM","colab_type":"code","colab":{}},"source":["train_sp = spark.createDataFrame(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0VgHulrCERJ","colab_type":"code","colab":{}},"source":["train_sp = train_sp.toDF(*['c0', 'c1', 'c2', 'c3', 'target'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Df8znQ98eAYf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"status":"ok","timestamp":1587156783058,"user_tz":-330,"elapsed":36282,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"960a4363-efd1-47dd-c3dc-0bd5f1689d17"},"source":["train_sp.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+------+\n","|                  c0|                  c1|                  c2|                  c3|target|\n","+--------------------+--------------------+--------------------+--------------------+------+\n","| -1.8873649371603203| -1.1455691898002351|  0.8396761000312767|  -2.008855708086743|     0|\n","|-0.18266809216582025|-0.12226678277057923| 0.08251252435219325| -0.2054662226628986|     0|\n","| -0.7315948349672106|  0.6559036936823883| 0.20531124349308186| 0.28714060384238044|     0|\n","| -0.7749652163170958|  0.7440265567629247| 0.21210307064010814| 0.35187466485355684|     0|\n","| -1.3394227045324436| -1.0424630852864827|  0.6209706165480363| -1.6479989577746343|     0|\n","|-0.18017499772535683| -1.6244897462240875| 0.24568656908689943| -1.6598910893214862|     1|\n","|    0.82509156468382| -0.5728119479422702| -0.2497852096620914|-0.16210051159032354|     0|\n","|  1.1781029789531323| 0.17113092183663403| -0.4647047960767629|  0.7268761264280708|     1|\n","| -1.2057458102820222|  1.4411211027600361|  0.2990313154948841|  0.8221866229867812|     0|\n","|-0.23066810781360036|-0.18246194798172555| 0.10726079558688516|-0.28665299425314167|     0|\n","| -1.0072234845517323|  0.8545722498639476| 0.28795478249968537|   0.348380613815614|     0|\n","| 0.47538225794367417|  1.7486569358975648| -0.3710120313222654|  1.9207939550154742|     1|\n","| -0.2131515767790706|  -1.748906639130869| 0.27176346054110195| -1.7961522233276521|     1|\n","| -1.5496574364883247|   1.775625586462539|  0.3926854919815291|  0.9825290487330922|     0|\n","| -1.1086769809797885|  1.3756666418155143|   0.269433691991313|   0.804990744964285|     0|\n","|  1.5405802242318427| -0.7978912930416052|-0.49606651071854874|-0.03945467385592581|     1|\n","|  0.2892416515109355| -0.6920923470206011|-0.03389062031080514| -0.5328709046924676|     0|\n","| 0.29676943127701305| -1.3502191165401722| 0.03515985815615573| -1.1669923954311183|     1|\n","|  1.3199355367253436|     1.4688067175103| -0.6601711724555561|   2.051833559881125|     1|\n","|  -2.125342086258849|  2.3959332555912507|  0.5428607911795114|   1.309428138106237|     0|\n","+--------------------+--------------------+--------------------+--------------------+------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vDOj98vWeKfa","colab_type":"text"},"source":["* Replicate the Dataset n times"]},{"cell_type":"code","metadata":{"id":"D68Aj3oaeC46","colab_type":"code","colab":{}},"source":["# replicate the spark dataframe into multiple copies\n","replication_df = spark.createDataFrame(pd.DataFrame(list(range(0,100)),columns=['replication_id']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgX3tYdLePdC","colab_type":"code","colab":{}},"source":["replicated_train_df = train_sp.crossJoin(replication_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6U659yieUga","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587156784888,"user_tz":-330,"elapsed":38045,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"94c40aa0-bd68-4e2a-8242-4b5b06b9f9d7"},"source":["print((replicated_train_df.count(), len(replicated_train_df.columns)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1000000, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oQSeW3XkepZn","colab_type":"text"},"source":["* Create Pandas UDF to run Model"]},{"cell_type":"code","metadata":{"id":"JQj1UM24enbt","colab_type":"code","colab":{}},"source":["# 0. Declare the schema for the output of our function\n","outSchema = StructType([StructField('replication_id',IntegerType(),True),StructField('Accuracy',DoubleType(),True),StructField('num_trees',IntegerType(),True),StructField('depth',IntegerType(),True),StructField('criterion',StringType(),True)])\n","\n","# decorate our function with pandas_udf decorator\n","@F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)\n","\n","def run_model(pdf):\n","  \n","    # 1. Get hyperparam values\n","    num_trees =  random.choice(list(range(50,500)))\n","    depth = random.choice(list(range(2,10)))\n","    criterion = random.choice(['gini','entropy'])\n","    replication_id = pdf.replication_id.values[0]\n","    \n","    # 2. Train test split\n","    X = pdf[['c0', 'c1', 'c2', 'c3']]\n","    y = pdf['target']\n","    #del X['target']\n","    Xtrain,Xcv,ytrain,ycv = train_test_split(X, y, test_size=0.33, random_state=42)\n","    \n","    # 3. Create model using the pandas dataframe\n","    clf = RandomForestClassifier(n_estimators=num_trees, max_depth = depth, criterion =criterion)\n","    clf.fit(Xtrain,ytrain)\n","    \n","    # 4. Evaluate the model\n","    accuracy = accuracy_score(clf.predict(Xcv),ycv)\n","    \n","    # 5. return results as pandas DF\n","    res =pd.DataFrame({'replication_id':replication_id,'Accuracy':accuracy, 'num_trees':num_trees,'depth':depth,'criterion':criterion}, index=[0])\n","    \n","    return res\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s4Fl1TUfevtA","colab_type":"text"},"source":["* Run the model"]},{"cell_type":"code","metadata":{"id":"Ldm-5lBYesoS","colab_type":"code","colab":{}},"source":["results = replicated_train_df.groupby(\"replication_id\").apply(run_model)\n","\n","results = results.sort(F.desc(\"Accuracy\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyVTcSkDe1Ek","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1587157027578,"user_tz":-330,"elapsed":280700,"user":{"displayName":"ml learning","photoUrl":"","userId":"11852164450280049527"}},"outputId":"f495e033-acc3-436c-cf61-9ff85809941d"},"source":["results = results.sort(F.desc(\"Accuracy\"))\n","results.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------+------------------+---------+-----+---------+\n","|replication_id|          Accuracy|num_trees|depth|criterion|\n","+--------------+------------------+---------+-----+---------+\n","|             6| 0.963030303030303|      491|    9|  entropy|\n","|            72| 0.963030303030303|      491|    9|  entropy|\n","|            31| 0.963030303030303|       94|    9|  entropy|\n","|            39| 0.963030303030303|       94|    9|  entropy|\n","|            68|0.9627272727272728|      428|    9|  entropy|\n","|            87|0.9627272727272728|      428|    9|  entropy|\n","|            61|0.9624242424242424|      131|    9|  entropy|\n","|             3|0.9624242424242424|      131|    9|  entropy|\n","|            19|0.9621212121212122|      336|    9|     gini|\n","|            29|0.9621212121212122|      336|    9|     gini|\n","|            51|0.9621212121212122|      363|    8|     gini|\n","|            27|0.9621212121212122|      363|    8|     gini|\n","|            26|0.9621212121212122|      101|    9|     gini|\n","|            65|0.9621212121212122|      101|    9|     gini|\n","|             1|0.9618181818181818|      306|    8|  entropy|\n","|            33|0.9618181818181818|      306|    8|  entropy|\n","|            57|0.9615151515151515|      326|    8|     gini|\n","|            12|0.9615151515151515|      435|    8|     gini|\n","|            50|0.9615151515151515|      326|    8|     gini|\n","|            55|0.9615151515151515|      435|    8|     gini|\n","+--------------+------------------+---------+-----+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yIJq4f-1y5Px","colab_type":"code","colab":{}},"source":["results.write.parquet('/content/drive/My Drive/Colab Notebooks/Models/Quora/Error_example.parquet.gzip')"],"execution_count":null,"outputs":[]}]}